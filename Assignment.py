# -*- coding: utf-8 -*-
"""Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12n7pfJpSBdk-TwNsXVTEoynAtxp-BdlO
"""

#importing library
import pandas as pd
import requests
from bs4 import BeautifulSoup
import numpy as np
from google.colab import files
import nltk
import string
nltk.download('punkt')

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_excel('/content/drive/MyDrive/Blackcoffer_Assignment/Input.xlsx')
df

df=df.iloc[0:100]
df

df.drop('URL_ID',axis=1,inplace=True)

url_id = 1
output_text = ""
list_number = 1

for i in range(0, len(df)):
    j = df.iloc[i].values
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'
    }
    page = requests.get(j[0], headers=headers)
    soup = BeautifulSoup(page.content, 'html.parser')
    content_elements = soup.find_all(attrs={'class': 'td-post-content tagdiv-type'})

    if len(content_elements) >= 1:
        content = content_elements[0]
        text_content = content.text.replace('\xa0', " ").replace('\n', " ")
        print(len(text_content))

        found_elements = content.find_all(['p', 'ol'])

        if found_elements:
            for element in found_elements:
                if element.name == 'p':
                    output_text += element.get_text() + ' '
                elif element.name == 'ol':
                    for li in element.find_all('li'):
                        output_text += f"{list_number}. {li.get_text()} "
                        list_number += 1  # Increment the list item number
        else:
            print("No paragraphs or ordered lists found.")
    else:
        print("Content not found or does not have at least one element.")

if not output_text.strip():
    print("No text found.")
else:
    print(output_text)

text=pd.read_csv("/content/100.txt",header=None)
text.info()

text.drop(1,axis=1,inplace=True)

text=text.astype(str)

#converting text to sentence
import re
a=text[0].str.split('([\.]\s)',expand=False)#splitting text on '.'
b=a.explode()#converting to rows
b=pd.DataFrame(b)#creating data frame
b.columns=['abc']

#removing . char from each rows
def abcd(x):
    nopunc =[char for char in x if char != '.']
    return ''.join(nopunc)
b['abc']=b['abc'].apply(abcd)

#replacing empty space with null values
c=b.replace('',np.nan,regex=True)
c=c.mask(c==" ")
c=c.dropna()
c.reset_index(drop=True,inplace=True)

c

punc=[punc for punc in string.punctuation]

punc

#importing stop words files that are provided
StopWords_Auditor=pd.read_csv("/content/drive/MyDrive/Blackcoffer_Assignment/StopWords_Auditor.txt",header=None)
StopWords_Currencies=pd.read_csv("/content/drive/MyDrive/Blackcoffer_Assignment/StopWords_Currencies.txt",header=None,encoding='ISO-8859-1',delimiter='\t')
StopWords_DatesandNumbers=pd.read_csv("/content/drive/MyDrive/Blackcoffer_Assignment/StopWords_DatesandNumbers.txt",header=None)
StopWords_Generic=pd.read_csv("/content/drive/MyDrive/Blackcoffer_Assignment/StopWords_GenericLong.txt",header=None)
StopWords_GenericLong=pd.read_csv("/content/drive/MyDrive/Blackcoffer_Assignment/StopWords_GenericLong.txt",header=None)
StopWords_Geographic=pd.read_csv("/content/drive/MyDrive/Blackcoffer_Assignment/StopWords_Geographic.txt",header=None)
StopWords_Names=pd.read_csv("/content/drive/MyDrive/Blackcoffer_Assignment/StopWords_Names.txt",header=None)

#creating func for removing stop words
def text_process(text):
    nopunc =[char for char in text if char not in punc or char not in [':',',','(',')','’','?']]
    nopunc=''.join(nopunc)
    txt=' '.join([word for word in nopunc.split() if word.lower() not in StopWords_Auditor])
    txt1=' '.join([word for word in txt.split() if word.lower() not in StopWords_Currencies])
    txt2=' '.join([word for word in txt1.split() if word.lower() not in StopWords_DatesandNumbers])
    txt3=' '.join([word for word in txt2.split() if word.lower() not in StopWords_Generic])
    txt4=' '.join([word for word in txt3.split() if word.lower() not in StopWords_GenericLong])
    txt5=' '.join([word for word in txt4.split() if word.lower() not in StopWords_Geographic])
    return ' '.join([word for word in txt5.split() if word.lower() not in StopWords_Names])

#applying func for each row
c['abc']=c['abc'].apply(text_process)
c

#importing master Dictionary
positive=pd.read_csv("/content/drive/MyDrive/Blackcoffer_Assignment/positive-words.txt",header=None)
negative=pd.read_csv("/content/drive/MyDrive/Blackcoffer_Assignment/positive-words.txt",header=None,encoding="ISO-8859-1",delimiter='\t' )

positive.columns=['abc']
negative.columns=['abc']
positive['abc']=positive['abc'].astype(str)
negative['abc']=negative['abc'].astype(str)

#positive and negative dictionary without stopwords
positive['abc']=positive['abc'].apply(text_process)
negative['abc']=negative['abc'].apply(text_process)

#positive list
length=positive.shape[0]
post=[]
for i in range(0,length):
   nopunc =[char for char in positive.iloc[i] if char not in string.punctuation or char != '+']
   nopunc=''.join(nopunc)

   post.append(nopunc)

#negative list
length=negative.shape[0]
neg=[]
for i in range(0,length):
  nopunc =[char for char in negative.iloc[i] if char not in string.punctuation or char != '+']
  nopunc=''.join(nopunc)
  neg.append(nopunc)

#importing tokenize library
from nltk.tokenize import word_tokenize

txt_list=[]
length=c.shape[0]
for i in range(0,length):
  txt=' '.join([word for word in c.iloc[i]])
  txt_list.append(txt)

#tokenization of text
tokenize_text=[]
for i in txt_list:

  tokenize_text+=(word_tokenize(i))

print(tokenize_text)

len(tokenize_text)

positive_score=0
for i in tokenize_text:
  if(i.lower() in post):
    positive_score+=1
print('postive score=', positive_score)

negative_score=0
for i in tokenize_text:
  if(i.lower() in neg):
    negative_score+=1
print('negative score=', negative_score)

#Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)
Polarity_Score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001)
print('polarity_score=', Polarity_Score)

#Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)
subjectiivity_score=(positive_score-negative_score)/((len(tokenize_text))+ 0.000001)
print('subjectivity_score',subjectiivity_score)

length=c.shape[0]
avg_length=[]
for i in range(0,length):
  avg_length.append(len(c['abc'].iloc[i]))
avg_senetence_length=sum(avg_length)/len(avg_length)
print('avg sentence length=', avg_senetence_length)

vowels=['a','e','i','o','u']
import re
count=0
complex_Word_Count=0
for i in tokenize_text:
  x=re.compile('[es|ed]$')
  if x.match(i.lower()):
   count+=0
  else:
    for j in i:
      if(j.lower() in vowels ):
        count+=1
  if(count>2):
   complex_Word_Count+=1
  count=0

Percentage_of_Complex_words=complex_Word_Count/len(tokenize_text)
print('percentag of complex words= ',Percentage_of_Complex_words)

#Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)
Fog_Index = 0.4 * (avg_senetence_length + Percentage_of_Complex_words)
print('fog index= ',Fog_Index )

length=c.shape[0]
avg_length=[]
for i in range(0,length):
  a=[word.split( ) for word in c.iloc[i]]
  avg_length.append(len(a[0]))
  a=0
#avg
avg_no_of_words_per_sentence=sum(avg_length)/length
print("avg no of words per sentence= ",avg_no_of_words_per_sentence)

vowels=['a','e','i','o','u']
import re
count=0
complex_Word_Count=0
for i in tokenize_text:
  x=re.compile('[es|ed]$')
  if x.match(i.lower()):
   count+=0
  else:
    for j in i:
      if(j.lower() in vowels ):
        count+=1
  if(count>2):
   complex_Word_Count+=1
  count=0
print('complex words count=',  complex_Word_Count)

word_count=len(tokenize_text)
print('word count= ', word_count)

vowels=['a','e','i','o','u']
import re
count=0
for i in tokenize_text:
  x=re.compile('[es|ed]$')
  if x.match(i.lower()):
   count+=0
  else:
    for j in i:
      if(j.lower() in vowels ):
        count+=1
syllable_count=count
print('syllable_per_word= ',syllable_count)

pronouns=['i','we','my','ours','us' ]
import re
count=0
for i in tokenize_text:
  if i.lower() in pronouns:
   count+=1
personal_pronouns=count
print('personal pronouns= ',personal_pronouns )

count=0
for i in tokenize_text:
  for j in i:
    count+=1
avg_word_length=count/len(tokenize_text)
print('avg word= ', avg_word_length)

output_df = pd.read_excel('/content/drive/MyDrive/Blackcoffer_Assignment/Output Data Structure.xlsx')

# URL_ID 44 ,57 does not exists i,e. page does not exist, throughs 404 error
# so we are going to drop these rows from the table
output_df.drop([44-37,57-37], axis = 0, inplace=True)

# These are the required parameters
variables = [positive_score,
            negative_score,
            Polarity_Score,
            subjectiivity_score,
            avg_senetence_length,
            Percentage_of_Complex_words,
            Fog_Index,
            avg_no_of_words_per_sentence,
            word_count,
            syllable_count,
            personal_pronouns,
            avg_word_length]

# write the values to the dataframe
for i, var in enumerate(variables):
  output_df.iloc[:,i+2] = var

#now save the dataframe to the disk
output_df.to_csv('/content/drive/MyDrive/Blackcoffer_Assignment/Output Data Structure.xlsx')

